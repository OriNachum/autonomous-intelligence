# Tool Chaining and Auditing Improvement Plan

## Goal
Improve the reliability and observability of tool chaining in the conversation application.
Currently, the system mishandles tool calls by concatenating their arguments into the assistant's text response, leading to confusing conversation history and potential model hallucinations. It also lacks standard "tool" role messages in the history and detailed auditing of tool outputs.

## Diagnosis
The issue observed (concatenated JSON in response) is caused by `conversation_app/app.py`:
```python
full_response += tool_args_str
```
This appends the raw JSON arguments of tool calls to the assistant's content. Additionally, the system uses `system` messages to report tool results, rather than the standard `tool` role messages expected by modern function-calling models (like Llama 3.1/3.2).

## Proposed Changes

### 1. Fix Assistant Message Construction in `conversation_app/app.py`
- **Stop appending tool arguments to `full_response`**. The `content` of the assistant message should only contain the text generated by the model (e.g., thought process), not the tool arguments.
- **Construct proper Assistant Message**: When adding the assistant message to history, include the `tool_calls` field if tools were called.
    ```python
    assistant_msg = {
        "role": "assistant",
        "content": full_response if full_response else None,
        "tool_calls": [
            {
                "id": tool_call["id"],
                "type": "function",
                "function": tool_call["function"]
            }
            for tool_call in tool_calls_accumulated.values()
        ] if tool_calls_accumulated else None
    }
    self.messages.append(assistant_msg)
    ```

### 2. Standardize Tool History
- **Use `tool` Role**: Instead of adding `[System] Action completed` messages, add messages with `role: "tool"`.
- **Link to Tool Call ID**: Ensure each tool response message references the `tool_call_id`.
- **Update `process_message`**:
    - After executing a tool, immediately append the tool output to the history.
    - Format:
        ```python
        self.messages.append({
            "role": "tool",
            "tool_call_id": tool_id,
            "content": json.dumps(result)
        })
        ```

### 3. Enhance Auditing
- **Update `ConversationLogger`**:
    - Add `log_tool_output(tool_name, tool_call_id, output, duration_ms, success, error=None)` method.
- **Integrate into `app.py`**:
    - Measure execution time of each tool.
    - Log the full output/result of the tool call using the new logger method.
    - This ensures we have a clear audit trail of: Model calls Tool -> Tool Executed with Args -> Tool Returned Result.

### 4. Refine Action Handler
- **Update `ActionHandler.execute_tool`**:
    - Ensure it returns the *result* of the action script execution.
    - Currently, it might be firing and forgetting or returning a generic success. It needs to await the script and return the actual return value (e.g., from `dizzy.py`).

## Verification Plan

### Automated Tests
1.  **Mock Test**: Create a test script `tests/test_tool_chaining.py` that mocks the `vLLM` response with tool calls and asserts:
    - The assistant message in history does *not* contain the JSON args in `content`.
    - The assistant message contains the correct `tool_calls` list.
    - The history contains `tool` messages with the correct `tool_call_id` and output.
    - The logger received the `log_tool_output` call.

### Manual Verification
1.  **Run the App**: Start `conversation_app.py`.
2.  **Trigger Tool**: Speak a command like "Look at me and nod your head" (triggering multiple tools).
3.  **Inspect Logs**: Check `logs/conversation_audit_*.jsonl` to verify:
    - `model_response_finished` does not have concatenated JSON.
    - `tool_call_executed` is present.
    - `tool_output` (new event) is present with the result.
4.  **Check Context**: Verify the model continues the conversation naturally after the tools, indicating it understands the tool history.
