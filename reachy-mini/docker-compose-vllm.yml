services:
  cleanup:
    image: alpine:latest
    container_name: system-cleanup
    privileged: true
    command: sh -c "sync && echo 3 > /proc/sys/vm/drop_caches && echo 'Cache cleanup completed'"
    volumes:
      - /proc:/proc
    
  vllm-server-front:
    env_file:
      - .env
    image: nvcr.io/nvidia/vllm:25.10-py3
    container_name: vllm-llama-3.2-3b-server-front
    restart: unless-stopped
    ports:
      - "${VLLM_FRONT_PORT}:8100"  # Expose vLLM API server on port 8100
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}  # Set your Hugging Face token in .env file or export it
      # SET THIS IN YOUR DOCKER-COMPOSE ENVIRONMENT:
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      sh -c "
      hf auth login --token ${HF_TOKEN} --add-to-git-credential &&
      hf download ${MODEL_ID} --cache-dir /root/.cache/huggingface &&
      vllm serve ${MODEL_ID}
      --kv-cache-dtype fp8
      --max-num-seqs 1024
      --max-num-batched-tokens 32768
      --max-model-len 65536
      --tensor-parallel-size 1 
      --gpu-memory-utilization 0.4
      --enable-prefix-caching 
      --enable-chunked-prefill 
      --host 0.0.0.0 
      --trust-remote-code 
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
      --disable-fastapi-docs 
      --download-dir /data/models/huggingface 
      --port 8100
      "
    volumes:
      - ./data/huggingface_cache:/root/.cache/huggingface  # Cache models locally
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  vllm-server-action:
    env_file:
      - .env
    image: nvcr.io/nvidia/vllm:25.10-py3
    container_name: vllm-llama-3.2-3b-server-action
    restart: unless-stopped
    ports:
      - "${VLLM_ACTION_PORT}:8200"  # Expose vLLM API server on port 8200
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}  # Set your Hugging Face token in .env file or export it
      # SET THIS IN YOUR DOCKER-COMPOSE ENVIRONMENT:
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      sh -c "
      hf auth login --token ${HF_TOKEN} --add-to-git-credential &&
      hf download ${MODEL_ID} --cache-dir /root/.cache/huggingface &&
      vllm serve ${MODEL_ID}
      --kv-cache-dtype fp8
      --max-num-seqs 1024
      --max-num-batched-tokens 8192
      --max-model-len 16384
      --tensor-parallel-size 1 
      --gpu-memory-utilization 0.12
      --enable-prefix-caching 
      --enable-chunked-prefill 
      --host 0.0.0.0 
      --trust-remote-code 
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
      --disable-fastapi-docs 
      --download-dir /data/models/huggingface 
      --port 8200
      "
    volumes:
      - ./data/huggingface_cache:/root/.cache/huggingface  # Cache models locally
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  reachy-conversation:
    build:
      context: .
      dockerfile: conversation_app/Dockerfile
    container_name: reachy-conversation-app
    restart: unless-stopped
    network_mode: host  # To access localhost:8100 vLLM server and expose daemon on localhost:8000
    privileged: true  # Required for USB and audio device access
    devices:
      - /dev/bus/usb:/dev/bus/usb  # USB devices for robot connection
      - /dev/snd:/dev/snd  # Audio devices for microphone and TTS playback
    volumes:
      - .:/app  # Mount entire project
      - ./data/whisper_models:/data/models/whisper  # Mount whisper models (read-only)
      - /tmp/reachy_sockets:/tmp/reachy_sockets  # Shared socket directory (kept for compatibility)
      - /dev/shm:/dev/shm  # Shared memory for audio buffers
      - /dev:/dev  # Device access
    working_dir: /app
    environment:
      - PYTHONUNBUFFERED=1
      - REACHY_BASE_URL=http://localhost:8000  # Daemon URL (accessible via host network mode)
      - AUDIO_DEVICE=plughw:CARD=Audio,DEV=0  # Use Reachy Mini Audio for TTS output
      - MODEL_ID=${MODEL_ID}
      - VLLM_FRONT_URL=${VLLM_FRONT_URL}
      - VLLM_ACTION_URL=${VLLM_ACTION_URL}
      - AUDIO_DEVICE_NAME=${AUDIO_DEVICE_NAME:-Reachy}
      - LANGUAGE=${LANGUAGE:-en}
      - PYTHONPATH=/app
      - HF_TOKEN=${HF_TOKEN}
      - WHISPER_MODEL_PATH=/data/models/whisper/large-v3  # Use local Whisper large-v3 model
    command: >
      sh -c "
      hf auth login --token ${HF_TOKEN} --add-to-git-credential &&
      hf download Systran/faster-whisper-large-v3 --local-dir /data/models/whisper/large-v3 &&
      echo 'Whisper model downloaded' &&
      exec python3 -m conversation_app.app
      "
    depends_on:
      vllm-server-front:
        condition: service_healthy
      vllm-server-action:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "pgrep", "-f", "conversation_app.app"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

