# ğŸš€ Local-Codex: Your AI Development Companion
Welcome to Local-Codex, the enterprise-ready solution for AI-powered development assistance! This guide will help you get up and running with minimal fuss.

# # ğŸ“‹ Prerequisites
For Jetson Users Only
If you're working on NVIDIA Jetson hardware:

- Clone the jetson-containers repository (if not already installed):
```
git clone https://github.com/dusty-nv/jetson-containers
```
- This provides the necessary image for running on Jetson platforms
- Skip this step if not using Jetson hardware

## ğŸ”Œ API Compatibility
Local-Codex works with any Chat Completions API provider:
- âœ… Ollama (default in our docker setup)
- âœ… vLLM
- âœ… OpenAI
- âœ… Groq
- âœ… Any other Chat Completions API compatible service
Choose what works best for your home or organization's needs!

## ğŸ³ Docker Setup
Our Docker Compose includes two containers:
- Ollama Container: Runs the LLM service
  - Can be replaced with any other Chat Completions API
  - Skip this container if using an external API
- Python Container: Core application
  - Installs Local-Codex
  - Installs ORS (OpenAI Responses Server)
  - ORS adapts Chat Completions API to Responses API format

## ğŸ’» Local Installation (Recommended)
For the best experience, we recommend installing locally:

## ğŸŒŸ Ready to Go!
Once installed, you're all set to supercharge your development workflow!

Happy coding! ğŸ’ª

